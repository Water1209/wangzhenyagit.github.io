---
layout: post
title: 《Hadoop权威指南》读书笔记
category: 读书笔记
tags: Hadoop
---

刚出的第四版。

买了好多奥莱利（O'Reilly Media）出版的书，写的很清楚，至少如很多书名上写的，“权威”，很多作者甚至都是相关开源软件的重要贡献者。

但是这本Hadoop的书，虽然有700多页，但是很多并不详细，因为介绍的内容实在是太多了，序列化、MapReduce、Spark、Hbase等等。

部分内容还是不是很明白，必要时需要反复翻看。分问题整理下。

### 不仅仅是批处理 ###

“不仅仅是批处理”是书中的1.4章的标题。从一开始的时候，Hadoop主要指HDFS+MapReduce，而现在，Hadoop生态，包括了Hbase、Spark、YARN、Pig、Hive等等，而且在进一步的扩张。

### Why Hadoop？ ###

大的背景：

> 计算机硬盘的另一个发展趋势，寻址时间的提升远远不敌于传输速率。寻址是将磁头移动到特定硬盘位置进行读写操作的过程。它是导致硬盘操作延迟的主要原因，而传输速度取决于硬盘的带宽。

适用场景：

> RDBMS适用于索引后数据集的点查询和更新，建立索引的数据库系统能够提供对小规模数据的低延迟数据检索和快速更新。MapReduce适合一次写入、多次读取数据的应用，关系型数据库则更适合持续更新的数据集。

MapReduce是一种补充，而不是取代RDBMS。数据量大小上，一般的传统数据库处理GB级别的数据，而MapReduce处理PB级别的数据。

### data locality特性 ###

> Hadoop尽量在计算节点上存储数据，以实现数据的本地快速访问。数据本地化（data locality）特性是Hadoop数据处理的核心，并因为此而获得良好的性能。

原因如上，为了减少数据的带宽。

### MapReduce ###

> MapReduce的一个合理分块的大小趋向于HDFS的一个块大小，默认是128MB。如果分片跨越两个数据块，那么对于任何一个HDFS节点，基本上都不可能同时存储这两个数据块，因此分片中的部分数据需要通过网络传输到map任务运行的节点。与使用本地数据运行整个map任务相比，这种方法显然效率更低。

“基本上都不可能同时存储这两个数据块”是因为，场景的假设，集群中的机器比较多，加入集群只有两台机器，那么不同时有才是小概率事件。

> MapReduce将其输出写入本地磁盘，而非HDFS。这是为什么？因为map的输出是中间结果：该中间结果由reduce任务处理后才产生最终输出结果，而且一旦作业完成，map结果就可以删除。
> 
> 如果运行map任务的节点在将map中间结果传送给reduce任务之前失败，Hadoop将在另一个节点上重新运行这个map任务已再次构建map的中间结果。

根本问题还是IO问题，如果map任务写入HDFS，那么HDFS会产生额外的IO复制到其他机器上，而这些是临时的结果，会很快删除。即使在使用这些临时结果之前，存储的结果丢了，那么可以再次进行一次计算，这与ES中同步的时候，只同步文档，在进行此索引很类似，为了节省IO，牺牲些CPU。

> 集群上的可用带宽限制了MapReduce作业的数量，因为尽量避免map和reduce任务之间的数据传输是有利的。Hadoop允许用户针对map任务的输出指定一个combiner，combiner函数输出作为reduce函数的输入。

也是为了节省IO，可以由用户指定IO的路径，最优化。

### HDFS特性 ###

> HDFS的构建思路是这样的：一次写入、多次读取是最高效的访问模式。每次分析都将涉及该数据集的大部分数据甚至全部，**因此读取整个数据集的时间延迟比读取第一条记录的时间延迟更重要。**

> 要求低时间延迟数据访问的应用，例如几十毫秒范围，并不适合在HDFS上运行。记住，HDFS是为高数据吞吐量应用优化的，这可能会以提高时间延迟为代价的。

为了追求吞吐量而牺牲延时是一个非常常见的方法。例如Kafka中的push和pull默认都会缓存一定数据后进行，保证每次操作的数据为一批。

> 由于NameNode将文件系统的元数据存储在内存中，因此该文件系统所有存储的文件总数受限于NameNode节点的内存容量。根据经验，每个文件、目录和数据块存储信息大约占150字节。如果有100w个文件，且每个文件占一个数据块，那么至少需要300M的内存。尽管存储上百万文件是可行的，但是存储数十亿个文件就超出了当前硬件的能力。

其他的文件系统，如fastdfs起始也有tracker节点，ceph中没有中心节点，但是上面两个文件系统中，文件地址是通过文件id根据算法计算出来的，为什么HDFS没有这么搞呢？个人感觉是，为了效率


